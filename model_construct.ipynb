{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import NFD, Lowercase, StripAccents\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers import decoders\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4145210, 3730689, 331616, 82905)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('qn_sequences.txt','r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    lines = [ln for ln in lines if len(ln)>1]\n",
    "    train_lines, val_lines = train_test_split(lines,test_size=0.1,shuffle=True)\n",
    "    val_lines, test_lines = train_test_split(val_lines,test_size=0.2,shuffle=True)\n",
    "    \n",
    "len(lines), len(train_lines), len(val_lines), len(test_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([Lowercase()])\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "\n",
    "trainer = WordPieceTrainer(special_tokens=[\"[UNK]\",\"[PAD]\", \"[MASK]\", \"[SEP]\"],vocab_size=8192)\n",
    "bert_tokenizer.train_from_iterator(lines,trainer)\n",
    "bert_tokenizer.enable_padding(pad_id=bert_tokenizer.token_to_id('[PAD]'),length=128)\n",
    "bert_tokenizer.enable_truncation(128)\n",
    "\n",
    "base = Path('mlm-baby-bert/tokenizer',)\n",
    "base.mkdir(exist_ok=True,parents=True)\n",
    "bert_tokenizer.save(str(base / 'qn_sequences.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file('./mlm-baby-bert/tokenizer/qn_sequences.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "368:nam  607:quốc  224:sơn  189:hà  368:nam  851:đế  596:cư  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  1:[PAD]  "
     ]
    },
    {
     "data": {
      "text/plain": [
       "'nam quốc sơn hà nam đế cư'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tokenizer.encode('nam quốc sơn hà nam đế cư')\n",
    "for i,t in zip(a.ids,a.tokens):\n",
    "    print(f'{i}:{t} ',end=' ')\n",
    "tokenizer.decode(a.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1, 2, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id(\"[UNK]\"),tokenizer.token_to_id(\"[PAD]\"),tokenizer.token_to_id(\"[MASK]\"), tokenizer.token_to_id(\"[SEP]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Maskset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset:\n",
    "    def __init__(self,lines):\n",
    "        self.lines = lines\n",
    "    def __len__(self,):\n",
    "        return len(self.lines)\n",
    "    def __getitem__(self,idx):\n",
    "        line = self.lines[idx]\n",
    "        ids = tokenizer.encode(line).ids\n",
    "        labels = ids.copy()\n",
    "        return ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [torch.tensor(i[0]) for i in batch]\n",
    "    labels = [torch.tensor(i[1]) for i in batch]\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels)\n",
    "    # mask 15% of text leaving [PAD]\n",
    "    mlm_mask = torch.rand(input_ids.size()) < 0.15 * (input_ids!=1)\n",
    "    masked_tokens = input_ids * mlm_mask\n",
    "    labels[masked_tokens==0]=-100 # set all tokens except masked tokens to -100\n",
    "    input_ids[masked_tokens!=0]=2 # MASK TOKEN\n",
    "    return input_ids, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MLMDataset(lines)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=2,shuffle=True,collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[360, 296, 429, 235, 415, 296, 385, 211, 1363, 950]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.__getitem__(0)[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([387, 529, 484,   2, 330,   2,   2,   2,   2, 379,   2, 834,   2, 449,\n",
      "        396,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,   1,\n",
      "          1,   1])\n",
      "tensor([-100, -100, -100,  273, -100,  385,  277,  738,  199, -100,  538, -100,\n",
      "        1500, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])\n"
     ]
    }
   ],
   "source": [
    "i,l = next(iter(dl))\n",
    "print(i[0])\n",
    "print(l[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model construct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- straightforward simple implementation.\n",
    "\n",
    "- nn.LayerNorm replaced with RMSNorm which is preferred to by many.\n",
    "\n",
    "- It looks like BERT but it is not BERT. BERT is more complicated than this.\n",
    "\n",
    "- Only implementing the MLM part of BERT so no need of [CLS] and [SEP] tokens\n",
    "\n",
    "- Learned positional embeddings instead of sinusoidal in BERT.\n",
    "\n",
    "- We can have a mask for the encoder self-attention as well by masking out the pad tokens so \n",
    "\n",
    "- attention layers ignore the extra stuff.\n",
    "\n",
    "- For inference currently only supports batch size of 1.\n",
    "\n",
    "- After the encoder outputs pass through the dim->vocab Linear layer, the logits at the \n",
    "\n",
    "- position where the token was masked are softmaxed and then with argmax the token that's \n",
    "\n",
    "- supposed to be there is predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "out: 1 x 128 x 256\n",
    "\n",
    "if the input sequence for inference was masked at position 4, we extract 1 x 256 at index 4:\n",
    "\n",
    "preds: out[:,4,:]\n",
    "\n",
    "softmax -> argmax\n",
    "\n",
    "preds: predicted token\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d, p=-1., eps=1e-8, bias=False):\n",
    "        \"\"\"\n",
    "            Root Mean Square Layer Normalization\n",
    "        :param d: model size\n",
    "        :param p: partial RMSNorm, valid value [0, 1], default -1.0 (disabled)\n",
    "        :param eps:  epsilon value, default 1e-8\n",
    "        :param bias: whether use bias term for RMSNorm, disabled by\n",
    "            default because RMSNorm doesn't enforce re-centering invariance.\n",
    "        \"\"\"\n",
    "        super(RMSNorm, self).__init__()\n",
    "\n",
    "        self.eps = eps\n",
    "        self.d = d\n",
    "        self.p = p\n",
    "        self.bias = bias\n",
    "\n",
    "        self.scale = nn.Parameter(torch.ones(d))\n",
    "        self.register_parameter(\"scale\", self.scale)\n",
    "\n",
    "        if self.bias:\n",
    "            self.offset = nn.Parameter(torch.zeros(d))\n",
    "            self.register_parameter(\"offset\", self.offset)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.p < 0. or self.p > 1.:\n",
    "            norm_x = x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = self.d\n",
    "        else:\n",
    "            partial_size = int(self.d * self.p)\n",
    "            partial_x, _ = torch.split(x, [partial_size, self.d - partial_size], dim=-1)\n",
    "\n",
    "            norm_x = partial_x.norm(2, dim=-1, keepdim=True)\n",
    "            d_x = partial_size\n",
    "\n",
    "        rms_x = norm_x * d_x ** (-1. / 2)\n",
    "        x_normed = x / (rms_x + self.eps)\n",
    "\n",
    "        if self.bias:\n",
    "            return self.scale * x_normed + self.offset\n",
    "\n",
    "        return self.scale * x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.n_heads = n_heads\n",
    "        assert dim % n_heads == 0, 'dim should be div by n_heads'\n",
    "        self.head_dim = self.dim // self.n_heads\n",
    "        self.in_proj = nn.Linear(dim,dim*3,bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "        self.out_proj = nn.Linear(dim,dim)\n",
    "        \n",
    "    def forward(self,x,mask=None):\n",
    "        b,t,c = x.shape\n",
    "        q,k,v = self.in_proj(x).chunk(3,dim=-1)\n",
    "        q = q.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        k = k.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        v = v.view(b,t,self.n_heads,self.head_dim).permute(0,2,1,3)\n",
    "        \n",
    "        qkT = torch.matmul(q,k.transpose(-1,-2)) * self.scale\n",
    "        qkT = self.attn_dropout(qkT)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.to(dtype=qkT.dtype,device=qkT.device)\n",
    "            qkT = qkT.masked_fill(mask==0,float('-inf'))\n",
    "              \n",
    "        qkT = F.softmax(qkT,dim=-1)\n",
    "        attn = torch.matmul(qkT,v)\n",
    "        attn = attn.permute(0,2,1,3).contiguous().view(b,t,c)\n",
    "        out = self.out_proj(attn)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self,dim,dropout=0.):\n",
    "        super().__init__()\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(dim,dim*4),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(dim*4,dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feed_forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, n_heads, attn_dropout=0., mlp_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.attn = MultiheadAttention(dim,n_heads,attn_dropout)\n",
    "        self.ffd = FeedForward(dim,mlp_dropout)\n",
    "        self.ln_1 = RMSNorm(dim)\n",
    "        self.ln_2 = RMSNorm(dim)\n",
    "        \n",
    "    def forward(self,x,mask=None):\n",
    "        x = self.ln_1(x)\n",
    "        x = x + self.attn(x,mask)\n",
    "        x = self.ln_2(x)\n",
    "        x = x + self.ffd(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,max_len,dim):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.class_embedding = nn.Embedding(vocab_size,dim)\n",
    "        self.pos_embedding = nn.Embedding(max_len,dim)\n",
    "    def forward(self,x):\n",
    "        x = self.class_embedding(x)\n",
    "        pos = torch.arange(0,x.size(1),device=x.device)\n",
    "        x = x + self.pos_embedding(pos)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMBERT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = Embedding(config['vocab_size'],config['max_len'],config['dim'])\n",
    "        \n",
    "        self.depth = config['depth']\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(\n",
    "                dim=config['dim'],\n",
    "                n_heads=config['n_heads'],\n",
    "                attn_dropout=config['attn_dropout'],\n",
    "                mlp_dropout=config['mlp_dropout']\n",
    "            ) for _ in range(self.depth)\n",
    "        ])\n",
    "        \n",
    "        self.ln_f = RMSNorm(config['dim'])\n",
    "        \n",
    "        self.mlm_head = nn.Linear(config['dim'],config['vocab_size'],bias=False)\n",
    "        \n",
    "        self.embedding.class_embedding.weight = self.mlm_head.weight # weight tying\n",
    "        \n",
    "        self.pad_token_id = config['pad_token_id']\n",
    "        self.mask_token_id = config['mask_token_id']\n",
    "        self.sep_token_id = config['sep_token_id']\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def create_src_mask(self,src):\n",
    "        return (src != self.pad_token_id).unsqueeze(1).unsqueeze(2) # N, 1, 1, src_len\n",
    "    \n",
    "    def forward(self,input_ids,labels=None):\n",
    "        \n",
    "        src_mask = self.create_src_mask(input_ids)\n",
    "        enc_out = self.embedding(input_ids)\n",
    "        for layer in self.encoders:\n",
    "            enc_out = layer(enc_out,mask=src_mask)\n",
    "        \n",
    "        enc_out = self.ln_f(enc_out)\n",
    "        \n",
    "        logits = self.mlm_head(enc_out)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.cross_entropy(logits.view(-1,logits.size(-1)),labels.view(-1))\n",
    "            return {'loss': loss, 'logits': logits}\n",
    "        else:\n",
    "            # assuming inference input_ids only have 1 [MASK] token\n",
    "            mask_idx = (input_ids==self.mask_token_id).flatten().nonzero().item()\n",
    "            mask_preds = F.softmax(logits[:,mask_idx,:],dim=-1).argmax(dim=-1)\n",
    "            return {'mask_predictions':mask_preds}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'dim': 256,\n",
    "    'n_heads': 8,\n",
    "    'attn_dropout': 0.1,\n",
    "    'mlp_dropout': 0.1,\n",
    "    'depth': 6,\n",
    "    'vocab_size': 8192,\n",
    "    'max_len': 128,\n",
    "    'pad_token_id': 1,\n",
    "    'mask_token_id': 2,\n",
    "    'sep_token_id' : 3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLMBERT(config).to('cuda')\n",
    "print('trainable:',sum([p.numel() for p in model.parameters() if p.requires_grad]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MLMDataset(train_lines)\n",
    "val_ds = MLMDataset(val_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single Masking Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST : SINGLE TOKEN MASKING\n",
    "\n",
    "test_actuals = []\n",
    "test_batches = []\n",
    "for ln in tqdm(test_lines):\n",
    "    tokenized = tokenizer.encode(ln)\n",
    "    fi = len(tokenized.ids)\n",
    "    if 1 in tokenized.special_tokens_mask:\n",
    "        fi = torch.tensor(tokenized.special_tokens_mask).nonzero()[0].item() # ignore [PAD]\n",
    "    m = torch.randint(0,fi,(1,)).item() # select random token to mask\n",
    "    input_ids = torch.tensor(tokenized.ids)\n",
    "    test_actuals.append(input_ids[m].item())\n",
    "    input_ids[m]=2 # replace with [MASK]\n",
    "    test_batches.append(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "test_accuracies = []\n",
    "best_val_loss = 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(),lr=6e-4 / 25.)\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optim,max_lr=6e-4,steps_per_epoch=len(train_dl),epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in tqdm(range(epochs)):\n",
    "    model.train()\n",
    "    trl = 0.\n",
    "    tprog = tqdm(enumerate(train_dl),total=len(train_dl))\n",
    "    for i, (input_ids, labels) in tprog:\n",
    "        input_ids = input_ids.to('cuda')\n",
    "        labels = labels.to('cuda')\n",
    "        loss = model(input_ids,labels)['loss']\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        optim.zero_grad()\n",
    "        sched.step()\n",
    "        trl += loss.item()\n",
    "        tprog.set_description(f'train step loss: {loss.item():.4f}')\n",
    "    train_losses.append(trl/len(train_dl))\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        vrl = 0.\n",
    "        vprog = tqdm(enumerate(val_dl),total=len(val_dl))\n",
    "        for i, (input_ids, labels) in vprog:\n",
    "\n",
    "            input_ids = input_ids.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            loss = model(input_ids,labels)['loss']\n",
    "            vrl += loss.item()\n",
    "            vprog.set_description(f'valid step loss: {loss.item():.4f}')\n",
    "        vloss = vrl/len(val_dl)\n",
    "        valid_losses.append(vloss)\n",
    "        print(f'epoch {ep} | train_loss: {train_losses[-1]:.4f} valid_loss: {valid_losses[-1]:.4f}')\n",
    "        \n",
    "        if vloss < best_val_loss:\n",
    "            best_val_loss = vloss\n",
    "            print('PREDICTING!')\n",
    "            test_predictions = []\n",
    "            for input_ids in tqdm(test_batches):\n",
    "                input_ids = input_ids.unsqueeze(0)\n",
    "                input_ids = input_ids.to('cuda')\n",
    "                mask_preds = model(input_ids)['mask_predictions']\n",
    "                test_predictions.extend(list(mask_preds.detach().cpu().flatten().numpy()))\n",
    "            \n",
    "            tacc = accuracy_score(test_actuals, test_predictions)\n",
    "            test_accuracies.append(tacc)\n",
    "            print(f'SINGLE MASK TOKEN PREDICTION ACCURACY: {tacc:.4f}')\n",
    "            print('saving best model...')\n",
    "            sd = model.state_dict()\n",
    "            torch.save(sd,'./mlm-baby-bert/model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses,color='red',label='train loss')\n",
    "plt.plot(valid_losses,color='orange',label='valid loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(test_accuracies)\n",
    "plt.title('single mask token prediction accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model\n",
    "sd = torch.load('./mlm-baby-bert/model.pt')\n",
    "model.load_state_dict(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(sentence):\n",
    "    \n",
    "    x = tokenizer.encode(sentence)\n",
    "    \n",
    "    # picking an index to mask, range: [0,len-1]\n",
    "    fi = len(x.ids)\n",
    "    # if the sample contains pad tokens, we can't mask them, so limiting the end index to ignore padding\n",
    "    if 1 in x.special_tokens_mask:\n",
    "        fi = torch.tensor(x.special_tokens_mask).nonzero()[0].item() # ignore [PAD]\n",
    "    # random index to mask\n",
    "    idx = torch.randint(0,fi,(1,)).item()\n",
    "    \n",
    "    input_ids = x.ids.copy()\n",
    "    masked_token = tokenizer.decode([input_ids[idx]])\n",
    "    \n",
    "    # masking\n",
    "    input_ids[idx] = 2 # idx -> [MASK]\n",
    "    masked_sentence = input_ids.copy()\n",
    "    \n",
    "    # preparing input\n",
    "    input_ids = torch.tensor(input_ids,dtype=torch.long).unsqueeze(0).to('cuda')\n",
    "    \n",
    "    # extracting the predicted token\n",
    "    out = model(input_ids)\n",
    "    predicted = x.ids.copy()\n",
    "    predicted[idx] = out['mask_predictions'].item()\n",
    "    predicted_token = tokenizer.decode([out['mask_predictions'].item()])\n",
    "    \n",
    "    print(f'masked: {masked_token} predicted: {predicted_token}')\n",
    "    masked_sentence = tokenizer.decode(masked_sentence,skip_special_tokens=False)\n",
    "    masked_sentence = masked_sentence.replace('[PAD]','')\n",
    "    print('ACTUAL:',sentence)\n",
    "    print('MASKED:',masked_sentence)\n",
    "    print(' MODEL:',tokenizer.decode(predicted))\n",
    "    \n",
    "    return int(masked_token == predicted_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for sentence in random.choices(train_lines+test_lines,k=100):\n",
    "    correct += predict_mask(sentence)\n",
    "    print('\\n\\n')\n",
    "print(f'CORRECT:{correct}/{100}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
