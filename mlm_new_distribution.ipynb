{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertConfig, BertForMaskedLM, get_linear_schedule_with_warmup\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"qn_sequences.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    sentences = [line.strip() for line in f if len(line.strip().split()) > 0]\n",
    "#sentences = sentences[50000]\n",
    "\n",
    "# Build vocab\n",
    "vocab = {}\n",
    "special_tokens = ['<unk>', '<pad>', '<sos>', '<eos>', '<mask>']\n",
    "for token in special_tokens:\n",
    "    vocab[token] = len(vocab)\n",
    "\n",
    "word_counter = Counter()\n",
    "for sent in sentences:\n",
    "    word_counter.update(sent.split())\n",
    "\n",
    "for word, _ in word_counter.most_common():\n",
    "    if word not in vocab:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "reverse_vocab = {idx: word for word, idx in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "        self.unk_token = '<unk>'\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = '<sos>'\n",
    "        self.eos_token = '<eos>'\n",
    "        self.mask_token = '<mask>'\n",
    "        self.max_len = 32\n",
    "\n",
    "    def encode(self, text, add_special_tokens=True):\n",
    "        tokens = text.split()\n",
    "        if add_special_tokens:\n",
    "            tokens = [self.sos_token] + tokens + [self.eos_token]\n",
    "        return [self.vocab.get(t, self.vocab[self.unk_token]) for t in tokens]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return ' '.join([self.reverse_vocab.get(i, self.unk_token) for i in ids])\n",
    "\n",
    "    def pad(self, ids, max_length):\n",
    "        return ids[:max_length] + [self.vocab[self.pad_token]] * max(0, max_length - len(ids))\n",
    "\n",
    "tokenizer = SimpleTokenizer(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset \n",
    "class MLMDataset(Dataset):\n",
    "    def __init__(self, sentences, tokenizer, max_len=32, mask_prob=0.20, prob_org = 0.2 ,seed = 42): # sentences: list of sentences\n",
    "        self.sentences = sentences\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mask_prob = mask_prob\n",
    "\n",
    "        self.blue_group = []\n",
    "        self.orange_group = []\n",
    "        \n",
    "        self.threshold = 0\n",
    "        \n",
    "        self.mask_blue = Counter()\n",
    "        self.mask_orange = Counter()\n",
    "        self.merge_counter = Counter()\n",
    "        \n",
    "        self.seed = seed\n",
    "        \n",
    "        self.prob_org = prob_org\n",
    "\n",
    "        self.word_counter_init()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx, printed = False):\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        # Get one sentence\n",
    "        text = self.sentences[idx] # 'nam quốc sơn hà nam đế cư'\n",
    "        input_ids = self.tokenizer.encode(text) # [2, 86, 278, 12, 11, 86, 498, 293, 3]\n",
    "        input_ids = self.tokenizer.pad(input_ids, self.max_len) # [2, 86, 278, 12, 11, 86, 498, 293, 3, 1, 1,..., 1]\n",
    "        if printed:\n",
    "            print('Input ids:',input_ids)\n",
    "\n",
    "\n",
    "        labels = [-100] * len(input_ids) # [-100, -100, -100, ... , -100]\n",
    "        masked_input_ids = input_ids.copy() # [2, 86, 278, 12, 11, 86, 498, 293, 3, 1, 1,..., 1]\n",
    "\n",
    "        candidate_indices = [\n",
    "            i for i in range(1, len(input_ids) - 1)\n",
    "            if input_ids[i] not in [\n",
    "                self.tokenizer.vocab[self.tokenizer.pad_token],\n",
    "                self.tokenizer.vocab[self.tokenizer.sos_token],\n",
    "                self.tokenizer.vocab[self.tokenizer.eos_token]\n",
    "            ]\n",
    "        ] # [1, 2, 3, 4, 5, 6, 7]: indices of maskable token\n",
    "        num_to_mask = max(1, int(self.mask_prob * len(candidate_indices))) # Number of token to mask\n",
    "        if printed:\n",
    "            print('Number of token masked:',num_to_mask)\n",
    "        mask_indices = random.sample(candidate_indices, min(len(candidate_indices), num_to_mask)) # List of indices of random token to mask [2,3,5,..]\n",
    "        if printed:\n",
    "            print('Random index mask token list:',mask_indices)\n",
    "\n",
    "        for i in mask_indices: # Go through each each token in random-maskable token list\n",
    "            original_token = input_ids[i]\n",
    "            prob = random.random()\n",
    "\n",
    "            if original_token in self.blue_group:\n",
    "                if prob < 0.9:\n",
    "                    masked_input_ids[i] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                else:\n",
    "                    masked_input_ids[i] = original_token\n",
    "                labels[i] = original_token\n",
    "            elif original_token in self.orange_group:\n",
    "                prob_org = random.random()\n",
    "                if prob_org < self.prob_org: # 0.01 ,0.05, 0.2 or 0.4 but it feel heuristic \n",
    "                    masked_input_ids[i] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                    labels[i] = original_token\n",
    "                else:\n",
    "                    mask_replace = [tok for tok in candidate_indices if tok not in mask_indices] # List of idx of token not in random-maskable token list\n",
    "                    if printed:\n",
    "                        print('Index of token not in random-maskable:', mask_replace)\n",
    "            \n",
    "                    mask_replace = [tok for tok in mask_replace if input_ids[tok] in self.blue_group] # List of idx of token in the mask_replace that in Blue group\n",
    "                    if printed:\n",
    "                        print('Index of token not in random-maskable and in Blue group:',mask_replace)\n",
    "\n",
    "                    if (len(mask_replace) != 0):\n",
    "                        mask_replace = random.sample(mask_replace,1) # Get one token to mask\n",
    "                        mask_replace = mask_replace[0]\n",
    "                    else:\n",
    "                        mask_replace = i\n",
    "\n",
    "                    if printed:\n",
    "                        print('Sampling index:',mask_replace)\n",
    "\n",
    "                    masked_input_ids[mask_replace] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                    labels[mask_replace] = input_ids[mask_replace]\n",
    "\n",
    "        attention_mask = [1 if token != self.tokenizer.vocab[self.tokenizer.pad_token] else 0 for token in input_ids]\n",
    "\n",
    "        # Debug print (chỉ in vài sample đầu)\n",
    "        if (printed):\n",
    "            if idx < 2:\n",
    "                print(\"Original:\", tokenizer.decode(input_ids))\n",
    "                print(\"Masked  :\", tokenizer.decode(masked_input_ids))\n",
    "                print(\"Labels  :\", [reverse_vocab.get(l, '-') if l != -100 else '_' for l in labels])\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(masked_input_ids),\n",
    "            \"labels\": torch.tensor(labels),\n",
    "            \"attention_mask\": torch.tensor(attention_mask)\n",
    "        }\n",
    "    \n",
    "    def mask_stat(self, printed = False):\n",
    "        random.seed(self.seed)\n",
    "\n",
    "        for idx in range(len(self.sentences)):\n",
    "            # Get one sentence\n",
    "            text = self.sentences[idx] # 'nam quốc sơn hà nam đế cư'\n",
    "            input_ids = self.tokenizer.encode(text) # [2, 86, 278, 12, 11, 86, 498, 293, 3]\n",
    "            input_ids = self.tokenizer.pad(input_ids, self.max_len) # [2, 86, 278, 12, 11, 86, 498, 293, 3, 1, 1,..., 1]\n",
    "            if printed:\n",
    "                print('Input ids:',input_ids)\n",
    "\n",
    "\n",
    "            labels = [-100] * len(input_ids) # [-100, -100, -100, ... , -100]\n",
    "            masked_input_ids = input_ids.copy() # [2, 86, 278, 12, 11, 86, 498, 293, 3, 1, 1,..., 1]\n",
    "\n",
    "            candidate_indices = [\n",
    "                i for i in range(1, len(input_ids) - 1)\n",
    "                if input_ids[i] not in [\n",
    "                    self.tokenizer.vocab[self.tokenizer.pad_token],\n",
    "                    self.tokenizer.vocab[self.tokenizer.sos_token],\n",
    "                    self.tokenizer.vocab[self.tokenizer.eos_token]\n",
    "                ]\n",
    "            ] # [1, 2, 3, 4, 5, 6, 7]: indices of maskable token\n",
    "            num_to_mask = max(1, int(self.mask_prob * len(candidate_indices))) # Number of token to mask\n",
    "            if printed:\n",
    "                print('Number of token masked:',num_to_mask)\n",
    "            mask_indices = random.sample(candidate_indices, min(len(candidate_indices), num_to_mask)) # List of indices of random token to mask [2,3,5,..]\n",
    "            if printed:\n",
    "                print('Random index mask token list:',mask_indices)\n",
    "\n",
    "            for i in mask_indices: # Go through each each token in random-maskable token list\n",
    "                original_token = input_ids[i]\n",
    "                prob = random.random()\n",
    "\n",
    "                if original_token in self.blue_group:\n",
    "                    if prob < 0.9:\n",
    "                        masked_input_ids[i] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                        self.mask_blue.update([self.tokenizer.decode([original_token])])\n",
    "                    else:\n",
    "                        masked_input_ids[i] = original_token\n",
    "                    labels[i] = original_token\n",
    "                elif original_token in self.orange_group:\n",
    "                    prob_org = random.random()\n",
    "                    if prob_org < self.prob_org: # 0.01 ,0.05, 0.2 or 0.4 but it feel heuristic \n",
    "                        masked_input_ids[i] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                        labels[i] = original_token\n",
    "                        self.mask_orange.update([self.tokenizer.decode([original_token])])\n",
    "                    else:\n",
    "                        mask_replace = [tok for tok in candidate_indices if tok not in mask_indices] # List of idx of token not in random-maskable token list\n",
    "                        if printed:\n",
    "                            print('Index of token not in random-maskable:', mask_replace)\n",
    "                \n",
    "                        mask_replace = [tok for tok in mask_replace if input_ids[tok] in self.blue_group] # List of idx of token in the mask_replace that in Blue group\n",
    "                        if printed:\n",
    "                            print('Index of token not in random-maskable and in Blue group:',mask_replace)\n",
    "\n",
    "                        if (len(mask_replace) != 0):\n",
    "                            mask_replace = random.sample(mask_replace,1) # Get one token to mask\n",
    "                            mask_replace = mask_replace[0]\n",
    "                            self.mask_blue.update([self.tokenizer.decode([input_ids[mask_replace]])])\n",
    "                        else:\n",
    "                            mask_replace = i\n",
    "                            self.mask_orange.update([self.tokenizer.decode([input_ids[mask_replace]])]) \n",
    "\n",
    "                        if printed:\n",
    "                            print('Sampling index:',mask_replace)\n",
    "\n",
    "                        masked_input_ids[mask_replace] = self.tokenizer.vocab[self.tokenizer.mask_token]\n",
    "                        labels[mask_replace] = input_ids[mask_replace]\n",
    "\n",
    "        self.merge_counter = self.mask_blue + self.mask_orange\n",
    "    \n",
    "    def word_counter_init(self):\n",
    "        word_counter = Counter()\n",
    "        for sent in sentences:\n",
    "            word_counter.update(sent.split())\n",
    "        self.threshold = int(sum([word_counter[word] for word in word_counter]) / len(word_counter))\n",
    "\n",
    "        self.blue_group = [self.tokenizer.encode(word)[1:-1][0] for word in word_counter if word_counter[word] < self.threshold]\n",
    "        self.orange_group = [self.tokenizer.encode(word)[1:-1][0] for word in word_counter if word_counter[word] >= self.threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset \n",
    "\n",
    "# ['<unk>', '<pad>', '<sos>', '<eos>', '<mask>']\n",
    "\n",
    "train_sentences, val_sentences = train_test_split(sentences, test_size=0.2, random_state=42)\n",
    "train_dataset = MLMDataset(train_sentences, tokenizer)\n",
    "val_dataset = MLMDataset(val_sentences, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config\n",
    "config = BertConfig(\n",
    "    vocab_size=len(vocab),\n",
    "    hidden_size=768, # 512\n",
    "    num_hidden_layers=12, # 6\n",
    "    num_attention_heads=12, # 8\n",
    "    intermediate_size=1024,\n",
    "    max_position_embeddings=tokenizer.max_len,\n",
    "    pad_token_id=vocab['<pad>']\n",
    ")\n",
    "\n",
    "model = BertForMaskedLM(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and Scheduler\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "total_steps = len(train_loader) * 20\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=int(0.2 * total_steps),\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, loader):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(loader, desc=\"Training\"):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation function\n",
    "def eval_model(model, loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs.logits, dim=-1)\n",
    "            mask_positions = labels != -100\n",
    "            correct += ((preds == labels) & mask_positions).sum().item()\n",
    "            total += mask_positions.sum().item()\n",
    "\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    return total_loss / len(loader), acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "patience_counter = 0\n",
    "best_val_acc = 0\n",
    "min_delta = 0.002\n",
    "patience = 5\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    train_loss = train_epoch(model, train_loader)\n",
    "    val_loss, val_acc = eval_model(model, val_loader)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val Accuracy: {val_acc:.4f}\")\n",
    "    if val_acc - best_val_acc > min_delta:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        model.save_pretrained(\"bert_custom_mlm_best\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Save model and vocab\n",
    "# model.save_pretrained(\"./bert_custom_mlm_best\")\n",
    "with open(\"custom_vocab.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vocab, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_3.12.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
